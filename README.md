# ğŸ§  Real-Time Sign Language to Word Sentence Translation Using LSTM and MediaPipe Holistic

> **Final Year B.Tech Project (2024â€“2025)**  
> **Hindustan Institute of Technology and Science, Chennai**  
> **Department of Computer Science and Engineering**  

---

## ğŸ« International Conference Recognition
ğŸ“ Proudly presented at the **International Conference on Advances in Mathematics and its Applications (ICAMA 2025)**  
ğŸ“ Hosted by **Sri Sairam Engineering College, Chennai**  
ğŸ—“ï¸ **April 2025**  
Our project was selected for presentation for its **innovative approach in real-time AI-based sign language translation**, bridging accessibility gaps for the deaf and hard-of-hearing community.

---

## ğŸ“– Overview
This project introduces an **AI-driven system** that translates **real-time sign language gestures into meaningful text sentences** using **Long Short-Term Memory (LSTM)** networks and **MediaPipe Holistic**.  
It enhances **inclusive communication** by eliminating dependency on human interpreters through **deep learning and computer vision**.

---

## ğŸ¯ Objectives
- Develop a **real-time sign language recognition system** using LSTM and MediaPipe Holistic.  
- Recognize **dynamic hand gestures and facial expressions** in continuous sequences.  
- Provide **accurate, accessible, and scalable** communication support for hearing-impaired users.

---

## âš™ï¸ System Architecture
<img width="601" height="451" alt="image" src="https://github.com/user-attachments/assets/2b438557-3f4d-490b-8667-630185da623f" />


---

## ğŸ§© Technologies Used
| Technology | Purpose |
|-------------|----------|
| **Python** | Core programming language |
| **TensorFlow / Keras** | Deep learning model development |
| **OpenCV** | Real-time video frame capture |
| **MediaPipe Holistic** | Keypoint landmark extraction |
| **NumPy / Pandas** | Data processing and manipulation |
| **Flask / Streamlit** | Model deployment and user interface |

---

## ğŸ’» Hardware & Software Requirements
- **Hardware:** i7/Ryzen 7 processor, 8â€“16 GB RAM, HD Webcam, NVIDIA GPU (optional)  
- **Software:** Windows 10 / Ubuntu 20.04+, Python 3.8+, TensorFlow, MediaPipe, OpenCV

---

## ğŸš€ Implementation Workflow
1. **Data Collection** â€“ Captured gesture datasets using webcam.  
2. **Landmark Extraction** â€“ Used MediaPipe Holistic to extract hand, face, and pose keypoints.  
3. **Data Preprocessing** â€“ Normalized coordinates and organized sequences for training.  
4. **Model Building** â€“ Trained an LSTM neural network for temporal gesture recognition.  
5. **Real-Time Translation** â€“ Integrated trained model with OpenCV for live gesture recognition.  
6. **Sentence Formation** â€“ Combined recognized words to form coherent sentences in real-time.  

---

## ğŸ“Š Performance Highlights
| Metric | Result |
|---------|---------|
| **Accuracy** | 92.4% |
| **Precision** | 91.8% |
| **Recall** | 92.1% |
| **Sentence-Level Accuracy** | 86% |
| **Average Response Time** | ~70â€“80ms per gesture sequence |
<img width="746" height="610" alt="image" src="https://github.com/user-attachments/assets/09b0f331-9b62-4cc9-b31c-1af96bdc7921" />

---

## ğŸ§  Learning Outcomes
- Mastered **MediaPipe Holistic** and **TensorFlow LSTM** for real-time applications.  
- Learned **data preprocessing**, **sequence modeling**, and **live video inference**.  
- Strengthened **team leadership**, collaboration, and **project presentation skills**.  
- Gained international exposure by presenting the project at **ICAMA 2025**.  

---

## ğŸ‘¥ Team & Roles
| Name | Role | Contribution |
|------|------|--------------|
| **Mohd Azim I** | *Team Lead & MediaPipe Developer* | Built real-time landmark detection pipeline, dataset creation scripts, and coordinated technical integration. |
| **Vidurvel Prouchotte** | *Model Developer* | Designed and trained the LSTM model, optimized accuracy and validation workflows. |
| **Monish Aston C** | *Frontend & Deployment Engineer* | Developed user interface using Streamlit/Flask, integrated LSTM predictions for live gesture-to-text translation. |

---

## ğŸ† Achievements
- ğŸ§© Successfully built a **real-time AI-based sign language translator**.  
- ğŸ’¡ Presented the project at an **International Conference (ICAMA 2025)**.  
- âš™ï¸ Combined **Deep Learning + Computer Vision** for an end-to-end gesture recognition system.  
- ğŸŒ Contributed toward **inclusive AI technology** supporting accessibility and education.

---

## ğŸ”® Future Enhancements
- Expand to **multi-language support** (ASL, ISL, BSL).  
- Integrate **voice synthesis** for audio-based communication.  
- Optimize for **mobile and edge devices** using TensorFlow Lite.  
- Explore **Transformer and attention-based architectures** for continuous sign translation.  
- Add **user-customizable gestures** and **interactive learning feedback**.

---

## ğŸ“š References
- [MediaPipe Holistic â€“ Google AI Documentation](https://developers.google.com/mediapipe)  
- [TensorFlow & Keras Documentation](https://www.tensorflow.org/)  
- Roy et al., *Vision Transformers for Continuous Sign Language Recognition*, IEEE, 2024.  
- Patel et al., *Real-Time American Sign Language Detection Using YOLO-v9*, ICIP, 2024.

---

## ğŸ“¸ Demo Preview
Example:  
<img width="618" height="496" alt="image" src="https://github.com/user-attachments/assets/a8a17df8-3816-41bc-90c8-f389a665354c" />


---

> â€œMaking communication accessible for everyone through the power of Artificial Intelligence.â€

---

#### ğŸ‘‘ Developed by [**Mohd Azim I**]
ğŸ“ Final Year B.Tech CSE | Hindustan Institute of Technology and Science | 2025    
ğŸŒ Presented at *ICAMA 2025, Sri Sairam Engineering College, Chennai*
